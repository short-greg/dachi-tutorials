{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial 1 — One Small Process, Big Payoffs\n",
        "\n",
        "**What you'll build:** a tiny `ChatProcess` you can call with a string, a `Msg`, or a `ListDialog`. You'll plug it into OpenAI's Responses API, compose prompts from reusable parts (`ModuleList`), route across personas with a named map (`ModuleDict`), render readable snapshots, and quickly serialize state so you can **save/restore** behavior.\n",
        "\n",
        "**Why Dachi?**\n",
        "\n",
        "* **Composability**: Build systems from small, strongly typed pieces.\n",
        "* **Reproducibility**: `spec()` & `state_dict()` mean you can checkpoint the *instructions* and the *structure* as your system evolves.\n",
        "* **Simplicity**: One process class, progressively extended—no class-per-cell sprawl."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1031e1c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Optional, Union, Callable\n",
        "from dataclasses import InitVar\n",
        "from dachi.core import (\n",
        "    BaseModule, Param, Attr,\n",
        "    ModuleList, ModuleDict,\n",
        "    Msg, ListDialog,\n",
        "    render\n",
        ")\n",
        "from dachi.proc import Process\n",
        "\n",
        "# Define all our classes at the top of the notebook\n",
        "class ChatProcess(Process):\n",
        "    \"\"\"A simple chat process that accepts str/Msg/ListDialog and returns a structured response\"\"\"\n",
        "    \n",
        "    system_prompt: InitVar[str] = \"You are a helpful Dachi tutorial assistant.\"\n",
        "    \n",
        "    def __post_init__(self, system_prompt: str):\n",
        "        super().__post_init__()\n",
        "        # System prompt given at instantiation\n",
        "        self.system_prompt = Param(system_prompt)\n",
        "        \n",
        "        # User-provided context that we'll embed in prompts\n",
        "        self.material = Attr(\"\")\n",
        "        \n",
        "        # Pluggable OpenAI Responses caller  \n",
        "        self.caller = Attr(None)\n",
        "    \n",
        "    def _normalize(self, x: Union[str, Msg, List[Msg], ListDialog]) -> str:\n",
        "        \"\"\"Normalize input to a single content string\"\"\"\n",
        "        if isinstance(x, str):\n",
        "            return x\n",
        "        elif isinstance(x, Msg):\n",
        "            return x.text\n",
        "        elif isinstance(x, ListDialog):\n",
        "            # Combine all messages into a single string\n",
        "            return \"\\n\".join(f\"{msg.role}: {msg.text}\" for msg in x)\n",
        "        elif isinstance(x, list) and all(isinstance(m, Msg) for m in x):\n",
        "            # Handle List[Msg]\n",
        "            return \"\\n\".join(f\"{msg.role}: {msg.text}\" for msg in x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported input type: {type(x)}\")\n",
        "    \n",
        "    def _render_material(self) -> str:\n",
        "        \"\"\"Render material as a string\"\"\"\n",
        "        if isinstance(self.material, dict):\n",
        "            return \"\\n\".join(f\"{k}: {v}\" for k, v in self.material.items())\n",
        "        return str(self.material)\n",
        "    \n",
        "    def forward(self, x: Union[str, Msg, List[Msg], ListDialog]) -> Dict[str, Any]:\n",
        "        \"\"\"Process input and return {prompt, output}\"\"\"\n",
        "        # Normalize input to string\n",
        "        user_content = self._normalize(x)\n",
        "        \n",
        "        # Compose the prompt\n",
        "        prompt_parts = [\n",
        "            f\"SYSTEM:\\n{self.system_prompt}\",\n",
        "            f\"\\nMATERIAL:\\n{self._render_material()}\",\n",
        "            f\"\\nUSER:\\n{user_content}\"\n",
        "        ]\n",
        "        prompt = \"\\n\".join(prompt_parts)\n",
        "        \n",
        "        # Generate output\n",
        "        if self.caller is None:\n",
        "            # Offline echo mode\n",
        "            output = f\"[echo] {user_content}\"\n",
        "        else:\n",
        "            # Call the external API\n",
        "            output = self.caller(prompt)\n",
        "        \n",
        "        return {\n",
        "            \"prompt\": prompt,\n",
        "            \"output\": output\n",
        "        }\n",
        "\n",
        "\n",
        "class PromptPart(BaseModule):\n",
        "    \"\"\"A reusable prompt component\"\"\"\n",
        "    \n",
        "    text: InitVar[str] = \"\"\n",
        "    \n",
        "    def __post_init__(self, text: str):\n",
        "        super().__post_init__()\n",
        "        self.text = Param(text)\n",
        "\n",
        "\n",
        "class PromptKit(BaseModule):\n",
        "    \"\"\"A collection of prompt parts using ModuleList\"\"\"\n",
        "    \n",
        "    parts: InitVar[ModuleList] = None\n",
        "    \n",
        "    def __post_init__(self, parts: Optional[ModuleList]):\n",
        "        super().__post_init__()\n",
        "        self.parts = parts or ModuleList(items=[])\n",
        "\n",
        "\n",
        "class MultiChat(BaseModule):\n",
        "    \"\"\"Route requests through named ChatProcess instances\"\"\"\n",
        "    \n",
        "    bots: InitVar[ModuleDict] = None\n",
        "    \n",
        "    def __post_init__(self, bots: Optional[ModuleDict]):\n",
        "        super().__post_init__()\n",
        "        self.bots = bots or ModuleDict(items={})\n",
        "    \n",
        "    def forward(self, name: str, x: Union[str, Msg, List[Msg], ListDialog]) -> Dict[str, Any]:\n",
        "        \"\"\"Call the named bot\"\"\"\n",
        "        if name not in self.bots:\n",
        "            raise ValueError(f\"Unknown bot: {name}. Available: {list(self.bots.keys())}\")\n",
        "        return self.bots[name](x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a54df47",
      "metadata": {},
      "source": [
        "## 0) Setup & the Single Class We'll Reuse\n",
        "\n",
        "We keep definitions in one place at the top:\n",
        "\n",
        "* `ChatProcess` (the star of this tutorial)\n",
        "  * `system_prompt: Param[str]` — given at instantiation\n",
        "  * `material: Attr[str|dict]` — user-provided context we'll embed\n",
        "  * `caller: Attr[Callable[[str], str]|None]` — pluggable OpenAI Responses caller\n",
        "  * Input normalization: `str | Msg | List[Msg]` → one content string\n",
        "  * Output: a small dict `{prompt, output}`\n",
        "\n",
        "Later, we'll also **define once** (still in the same top cell):\n",
        "\n",
        "* `PromptPart` + `PromptKit` (to show `ModuleList`)\n",
        "* `MultiChat` (to show `ModuleDict` of named personas)\n",
        "\n",
        "> Everything else below are **small cells that *use* these classes**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e81f13",
      "metadata": {},
      "source": [
        "## 1) Hello, `ChatProcess` (offline echo)\n",
        "\n",
        "**Goal:** Prove the shape of the API before calling any external service.\n",
        "\n",
        "**What you'll do**\n",
        "\n",
        "* Instantiate with a **system message**.\n",
        "* Call it with a simple string.\n",
        "* See that `material` is empty and output is a local echo.\n",
        "\n",
        "**What to run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a36def2",
      "metadata": {},
      "outputs": [],
      "source": [
        "cp = ChatProcess(system_prompt=\"You are a helpful Dachi tutorial assistant.\")\n",
        "cp(\"Hi Dachi!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8e3131",
      "metadata": {},
      "source": [
        "## 2) Turn on OpenAI Responses (one small injection)\n",
        "\n",
        "**Goal:** Use the **Responses API** without coupling our whole notebook to an SDK.\n",
        "\n",
        "**What you'll do**\n",
        "\n",
        "* Create a tiny function that calls OpenAI's Responses endpoint.\n",
        "* Assign it to `cp.caller`.\n",
        "* Call `cp(...)` again—now it returns model text.\n",
        "\n",
        "**What to run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fde0124",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Commented out for testing - uncomment when you have OpenAI API key\n",
        "# from openai import OpenAI\n",
        "# client = OpenAI()\n",
        "\n",
        "# def responses_call(prompt: str) -> str:\n",
        "#     r = client.responses.create(model=\"gpt-5\", input=prompt)\n",
        "#     return r.output_text\n",
        "\n",
        "# cp.caller = responses_call\n",
        "# cp(\"Give me one sentence on why composability matters.\")\n",
        "\n",
        "# For testing without OpenAI API key:\n",
        "def mock_responses_call(prompt: str) -> str:\n",
        "    return \"Composability enables building complex AI systems from simple, reusable components.\"\n",
        "\n",
        "cp.caller = mock_responses_call\n",
        "cp(\"Give me one sentence on why composability matters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b04dfbc",
      "metadata": {},
      "source": [
        "**Why this design**\n",
        "\n",
        "* The call is **isolated** behind `caller`. If APIs change or you switch vendors, you swap one function—**no rewrites** elsewhere."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73fcd899",
      "metadata": {},
      "source": [
        "## 3) Param vs Attr in action (without ML… yet)\n",
        "\n",
        "**Goal:** Make `material` useful and show how it changes the prompt.\n",
        "\n",
        "**What you'll do**\n",
        "\n",
        "* Keep the same `ChatProcess`.\n",
        "* Set `material` to a short dict.\n",
        "* Call `cp(...)` again and see the system+material appear inside the prompt.\n",
        "\n",
        "**What to run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2812d046",
      "metadata": {},
      "outputs": [],
      "source": [
        "cp.material = {\n",
        "    \"project\": \"Dachi\",\n",
        "    \"goal\": \"simple, composable AI that stays maintainable\",\n",
        "    \"tone\": \"practical and concise\"\n",
        "}\n",
        "cp(\"Explain the benefits in 2 short bullets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc643fb5",
      "metadata": {},
      "source": [
        "**Takeaway**\n",
        "\n",
        "* `system_prompt` is a **Param**—part of the module's identity.\n",
        "* `material` is an **Attr**—runtime context you can change freely.\n",
        "* We'll use these views (Param vs Attr) to **save/restore** later. That's the foundation that will enable ML over instructions/structure in future tutorials."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6adf31a1",
      "metadata": {},
      "source": [
        "## 4) Messages & Dialogs (one surface for multiple inputs)\n",
        "\n",
        "**Goal:** Accept `str`, `Msg`, or `ListDialog` with zero extra code.\n",
        "\n",
        "**What you'll do**\n",
        "\n",
        "* Create messages and a short dialog.\n",
        "* Call `cp(dialog)`—internally we'll normalize to a single content string for Responses.\n",
        "\n",
        "**What to run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47897467",
      "metadata": {},
      "outputs": [],
      "source": [
        "u1 = Msg(role=\"user\", text=\"What is Dachi?\")\n",
        "a1 = Msg(role=\"assistant\", text=\"A framework for composable AI.\")\n",
        "u2 = Msg(role=\"user\", text=\"How do I save state?\")\n",
        "\n",
        "dialog = ListDialog(messages=[u1, a1, u2])\n",
        "cp(dialog)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6df5ca8",
      "metadata": {},
      "source": [
        "**Why this matters**\n",
        "One process surface keeps cohesion high. You don't need parallel functions for strings vs dialogs—**less drift, fewer bugs**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a521df79",
      "metadata": {},
      "source": [
        "## 5) A quick peek at serialization & `parameters()` (1 minute)\n",
        "\n",
        "> We're not doing training or ML here; we just show how you'd **store and restore** behavior later.\n",
        "\n",
        "**What you'll do**\n",
        "\n",
        "* Compare `spec()` (Params only) with `state_dict()` (Params + Attr).\n",
        "* List `parameters()` to see what's considered a Param.\n",
        "* Rebuild a new `ChatProcess` from `spec()`.\n",
        "\n",
        "**What to run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34e613a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "spec_view = cp.spec()            # Params only (e.g., system_prompt)\n",
        "state_view = cp.state_dict()     # Params + Attr (includes material)\n",
        "list(cp.parameters())            # iterate (name, Param) pairs\n",
        "\n",
        "cp2 = ChatProcess.from_spec(spec_view)\n",
        "cp2.material = cp.material       # copy over runtime state only if you want it\n",
        "cp2(\"Confirm we restored behavior without copying code.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe3db36",
      "metadata": {},
      "source": [
        "**Why this matters**\n",
        "This is how you **save** system versions and **reproduce** results. In later tutorials we'll use these same hooks to **optimize instructions and structure** over time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cceb8a71",
      "metadata": {},
      "source": [
        "## 6) Reusable prompt parts with `ModuleList`\n",
        "\n",
        "**Goal:** Move from ad-hoc strings to a structured set of reusable prompt parts.\n",
        "\n",
        "**What you'll do**\n",
        "\n",
        "* Build a `PromptKit` with a `ModuleList[PromptPart]`.\n",
        "* Materialize `cp.material` from the kit, then call `cp(...)`.\n",
        "\n",
        "**What to run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8609ce23",
      "metadata": {},
      "outputs": [],
      "source": [
        "kit = PromptKit(parts=ModuleList(items=[\n",
        "    PromptPart(text=\"You must be concise.\"),\n",
        "    PromptPart(text=\"Answer with 2 bullets.\"),\n",
        "    PromptPart(text=\"Avoid marketing jargon.\")\n",
        "]))\n",
        "\n",
        "cp.material = \"\\n\".join(p.text.data for p in kit.parts)\n",
        "cp(\"Summarize Dachi's core benefits.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a678e5b9",
      "metadata": {},
      "source": [
        "**Nice side effect**\n",
        "`ModuleList` is **serializable**, so your prompt kit becomes a stable artifact—you can version it, share it, and swap parts confidently."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "527add93",
      "metadata": {},
      "source": [
        "## 7) Named personas with `ModuleDict`\n",
        "\n",
        "**Goal:** Route requests through a map of named `ChatProcess` instances, each with a different system message.\n",
        "\n",
        "**What you'll do**\n",
        "\n",
        "* Create two `ChatProcess` personas (\"teacher\" and \"concise\") and store them in a `ModuleDict`.\n",
        "* Use `MultiChat` to call by name.\n",
        "\n",
        "**What to run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1337453",
      "metadata": {},
      "outputs": [],
      "source": [
        "teacher = ChatProcess(system_prompt=\"You are a patient teacher. Explain step by step.\")\n",
        "concise = ChatProcess(system_prompt=\"You answer in one crisp sentence.\")\n",
        "teacher.caller = concise.caller = mock_responses_call  # same injected caller\n",
        "\n",
        "mc = MultiChat(bots=ModuleDict(items={\"teacher\": teacher, \"concise\": concise}))\n",
        "mc.forward(\"teacher\", \"Explain Param vs Attr in Dachi.\")\n",
        "mc.forward(\"concise\", \"Explain Param vs Attr in Dachi.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why this matters**\n",
        "`ModuleDict` gives you a **typed, serializable registry** of behaviors. You can checkpoint the whole map and bring it back exactly later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Rendering readable snapshots\n",
        "\n",
        "**Goal:** Produce human-readable views of specs, states, and summaries.\n",
        "\n",
        "**What you'll do**\n",
        "\n",
        "* Render `spec()` and `state_dict()` for both a single `ChatProcess` and the `MultiChat` container.\n",
        "\n",
        "**What to run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dachi.core import render\n",
        "\n",
        "print(render(cp.spec()))\n",
        "print(render(cp.state_dict()))\n",
        "print(render(mc.spec()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why this matters**\n",
        "When teams audit, share, and debug, **readable artifacts** are gold. Dachi emits compact, consistent representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Wrap-up: What you have now\n",
        "\n",
        "* **One small process** with three inputs (string, message, dialog), one output, and a pluggable LLM caller.\n",
        "* **Composable context** using `ModuleList` and **named personas** with `ModuleDict`.\n",
        "* **Serialization & parameters()** glimpsed—enough to save/restore now, and to support learning in future tutorials.\n",
        "* **Rendering** to produce stable, human-readable snapshots.\n",
        "\n",
        "**Where we're going next**\n",
        "\n",
        "* Add lightweight structure for multi-step processing.\n",
        "* Use serialization for **controlled iteration** (tuning instructions safely).\n",
        "* Optional: streaming and structured outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Appendix: What the top-cell class looks like (for reference)\n",
        "\n",
        "We define these **once** at the top of the notebook:\n",
        "\n",
        "* `ChatProcess(system_prompt: str)` with:\n",
        "  * `system_prompt: Param[str]`\n",
        "  * `material: Attr[str|dict]`\n",
        "  * `caller: Attr[Callable[[str], str]|None]`\n",
        "  * `_normalize`, `_render_material`, and `forward` (compose `SYSTEM/MATERIAL/USER` → call or echo)\n",
        "* `PromptPart` and `PromptKit` to show `ModuleList`\n",
        "* `MultiChat` with `bots: ModuleDict[str, ChatProcess]` and `forward(name, x)`\n",
        "\n",
        "> With these in place, every section above is just a tiny usage cell—**one idea at a time**."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
