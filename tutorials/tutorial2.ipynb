{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial 2 — From one worker to a parallel pipeline (async, streaming, and chunks)\n",
        "\n",
        "**What you'll build:** A small \"document insights\" pipeline that starts with one async worker and ends as a parallel, chunked, optionally streaming pipeline. It runs offline by default. If readers want, they can plug in their own LLM caller in one place without changing the rest of the code.\n",
        "\n",
        "**Why it matters:**\n",
        "\n",
        "* You learn **when to use** sync, async, and streaming processes.\n",
        "* You see how **chunking** and `async_process_map` boost throughput for I/O tasks.\n",
        "* You practice **composing stages** with `Sequential` and **aggregating results** with `reduce`.\n",
        "* You keep vendor code isolated, so APIs can change without rewrites.\n",
        "* You glimpse why serialization is important (save/restore later), without getting lost in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "None\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import time\n",
        "from typing import Any, Dict, List, Optional, Union, Callable, AsyncIterator\n",
        "\n",
        "from dachi.core import BaseModule, Param, Attr, InitVar\n",
        "from dachi.proc import (\n",
        "    Process, AsyncProcess, AsyncStreamProcess,\n",
        "    async_process_map, \n",
        "    Chunk, Recur,\n",
        "    reduce,\n",
        "    Sequential,\n",
        "\n",
        ")\n",
        "\n",
        "# Define all our classes at the top\n",
        "class DocInsights(AsyncProcess):\n",
        "    \"\"\"Async worker that analyzes text (mocked analysis by default)\"\"\"\n",
        "    \n",
        "    # Pluggable caller for real LLM integration\n",
        "    caller: Optional[Callable] = Attr(data=None)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.total_processed = 0\n",
        "    \n",
        "    async def aforward(self, text: Union[str, List[str]]) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze text and return insights dict\"\"\"\n",
        "        # Handle both single and batch inputs\n",
        "        texts = [text] if isinstance(text, str) else text\n",
        "        \n",
        "        results = []\n",
        "        for t in texts:\n",
        "            # Simulate async I/O work\n",
        "            await asyncio.sleep(0.1)\n",
        "            \n",
        "            if self.caller:\n",
        "                # Use real LLM if available\n",
        "                response = await self.caller(t)\n",
        "                # Parse response into our format\n",
        "                result = {\n",
        "                    \"sentiment\": \"positive\",  # Would parse from response\n",
        "                    \"topics\": [\"AI\", \"technology\"],\n",
        "                    \"word_count\": len(t.split())\n",
        "                }\n",
        "            else:\n",
        "                # Mock analysis\n",
        "                words = t.split()\n",
        "                word_count = len(words)\n",
        "                \n",
        "                # Simple sentiment based on keywords\n",
        "                positive_words = {\"good\", \"great\", \"excellent\", \"amazing\"}\n",
        "                negative_words = {\"bad\", \"poor\", \"terrible\", \"awful\"}\n",
        "                \n",
        "                text_lower = t.lower()\n",
        "                pos_score = sum(1 for w in positive_words if w in text_lower)\n",
        "                neg_score = sum(1 for w in negative_words if w in text_lower)\n",
        "                \n",
        "                if pos_score > neg_score:\n",
        "                    sentiment = \"positive\"\n",
        "                elif neg_score > pos_score:\n",
        "                    sentiment = \"negative\"\n",
        "                else:\n",
        "                    sentiment = \"neutral\"\n",
        "                \n",
        "                # Mock topics\n",
        "                topics = []\n",
        "                if \"ai\" in text_lower or \"artificial\" in text_lower:\n",
        "                    topics.append(\"AI\")\n",
        "                if \"data\" in text_lower:\n",
        "                    topics.append(\"data\")\n",
        "                if \"technology\" in text_lower or \"tech\" in text_lower:\n",
        "                    topics.append(\"technology\")\n",
        "                \n",
        "                result = {\n",
        "                    \"sentiment\": sentiment,\n",
        "                    \"topics\": topics,\n",
        "                    \"word_count\": word_count\n",
        "                }\n",
        "            \n",
        "            results.append(result)\n",
        "            self.total_processed += 1\n",
        "        \n",
        "        # Return single result or batch\n",
        "        return results[0] if isinstance(text, str) else results\n",
        "\n",
        "\n",
        "class DocStream(AsyncStreamProcess):\n",
        "    \"\"\"Streams per-chunk partial results and a final summary\"\"\"\n",
        "    \n",
        "    insights: DocInsights\n",
        "    \n",
        "    async def astream(self, text: str) -> AsyncIterator[Dict[str, Any]]:\n",
        "        \"\"\"Stream analysis results chunk by chunk\"\"\"\n",
        "        # Split into chunks for streaming\n",
        "        sentences = text.split('. ')\n",
        "        chunk_size = max(1, len(sentences) // 4)  # 4 chunks\n",
        "        \n",
        "        running_sentiment_scores = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n",
        "        all_topics = set()\n",
        "        total_words = 0\n",
        "        \n",
        "        for i in range(0, len(sentences), chunk_size):\n",
        "            chunk = '. '.join(sentences[i:i + chunk_size])\n",
        "            if not chunk:\n",
        "                continue\n",
        "                \n",
        "            # Analyze chunk\n",
        "            result = await self.insights.aforward(chunk)\n",
        "            \n",
        "            # Update running totals\n",
        "            running_sentiment_scores[result[\"sentiment\"]] += 1\n",
        "            all_topics.update(result[\"topics\"])\n",
        "            total_words += result[\"word_count\"]\n",
        "            \n",
        "            # Yield partial result\n",
        "            yield {\n",
        "                \"type\": \"chunk\",\n",
        "                \"chunk_id\": i // chunk_size,\n",
        "                \"progress\": min(100, ((i + chunk_size) / len(sentences)) * 100),\n",
        "                \"sentiment\": result[\"sentiment\"],\n",
        "                \"topics\": result[\"topics\"],\n",
        "                \"word_count\": result[\"word_count\"]\n",
        "            }\n",
        "        \n",
        "        # Yield final summary\n",
        "        dominant_sentiment = max(running_sentiment_scores.items(), key=lambda x: x[1])[0]\n",
        "        yield {\n",
        "            \"type\": \"final\",\n",
        "            \"sentiment\": dominant_sentiment,\n",
        "            \"topics\": list(all_topics),\n",
        "            \"word_count\": total_words,\n",
        "            \"chunk_count\": len(sentences) // chunk_size + (1 if len(sentences) % chunk_size else 0)\n",
        "        }\n",
        "\n",
        "\n",
        "class TextPrep(Process):\n",
        "    \"\"\"Tiny normalizer (trim, collapse whitespace)\"\"\"\n",
        "    \n",
        "    def forward(self, text: str) -> str:\n",
        "        \"\"\"Normalize text\"\"\"\n",
        "        # Trim and collapse whitespace\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "\n",
        "class ResultShape(Process):\n",
        "    \"\"\"Formats results for display\"\"\"\n",
        "    \n",
        "    format_type: str = Param(data=\"summary\")\n",
        "    \n",
        "    def forward(self, result: Dict[str, Any]) -> str:\n",
        "        \"\"\"Format result as string\"\"\"\n",
        "        if self.format_type == \"summary\":\n",
        "            return f\"Sentiment: {result['sentiment']}, Topics: {', '.join(result['topics'])}, Words: {result['word_count']}\"\n",
        "        elif self.format_type == \"json\":\n",
        "            import json\n",
        "            return json.dumps(result, indent=2)\n",
        "        else:\n",
        "            return str(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 — Hello, a single async worker\n",
        "\n",
        "**Goal:** Learn the shape of an `AsyncProcess`.\n",
        "\n",
        "* Run `DocInsights` on one small string.\n",
        "* Show a tiny, stable output (keys like `sentiment`, `topics`, `word_count`).\n",
        "* Mention: \"This runs offline. To use a real LLM, set `insights.caller = your_responses_function` later.\"\n",
        "\n",
        "**What to run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create our async worker\n",
        "insights = DocInsights()\n",
        "\n",
        "# Analyze a single document\n",
        "async def analyze_one():\n",
        "    result = await insights.aforward(\"This is a great example of AI technology in action. The data processing is excellent.\")\n",
        "    return result\n",
        "\n",
        "# Run it\n",
        "result = asyncio.run(analyze_one())\n",
        "print(f\"Analysis result: {result}\")\n",
        "print(f\"\\nThis runs offline. To use a real LLM, set `insights.caller = your_responses_function` later.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 — Concurrency with `asyncio` (quick win)\n",
        "\n",
        "**Goal:** Feel the benefit of async before we introduce chunking.\n",
        "\n",
        "* Call `insights` several times concurrently (e.g., `await asyncio.gather(...)`).\n",
        "* Print total elapsed time to show that parallel I/O work is faster than serial calls.\n",
        "\n",
        "**What they learn:** Async is for I/O-bound work; concurrency reduces wall-clock time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample documents\n",
        "docs = [\n",
        "    \"Artificial intelligence is transforming how we process data.\",\n",
        "    \"The technology sector shows great promise for innovation.\",\n",
        "    \"Machine learning algorithms are becoming more sophisticated.\",\n",
        "    \"Data science is essential for modern business decisions.\",\n",
        "    \"This is a terrible example of poor implementation.\"\n",
        "]\n",
        "\n",
        "# Serial execution\n",
        "async def serial_analysis():\n",
        "    start = time.time()\n",
        "    results = []\n",
        "    for doc in docs:\n",
        "        result = await insights.aforward(doc)\n",
        "        results.append(result)\n",
        "    elapsed = time.time() - start\n",
        "    return results, elapsed\n",
        "\n",
        "# Concurrent execution\n",
        "async def concurrent_analysis():\n",
        "    start = time.time()\n",
        "    tasks = [insights.aforward(doc) for doc in docs]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    elapsed = time.time() - start\n",
        "    return results, elapsed\n",
        "\n",
        "# Compare\n",
        "serial_results, serial_time = asyncio.run(serial_analysis())\n",
        "concurrent_results, concurrent_time = asyncio.run(concurrent_analysis())\n",
        "\n",
        "print(f\"Serial execution: {serial_time:.2f}s\")\n",
        "print(f\"Concurrent execution: {concurrent_time:.2f}s\")\n",
        "print(f\"Speedup: {serial_time/concurrent_time:.1f}x\")\n",
        "print(f\"\\nProcessed {len(docs)} documents. Total processed so far: {insights.total_processed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 — Chunking for throughput (`Chunk`, `Recur`)\n",
        "\n",
        "**Goal:** Move from \"many single calls\" to **chunked batches** and prepare for `async_process_map`.\n",
        "\n",
        "* Show `Chunk(data=docs, n=…)` with a few `n` values (small, medium, individual).\n",
        "* Use `Recur({...}, n=…)` once to pass the same config to each chunk (e.g., `include_topics=True`).\n",
        "\n",
        "**What they learn:** Batching is a dial for cost/latency; `Recur` broadcasts constants safely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create chunks of different sizes\n",
        "small_chunks = Chunk(data=docs, n=2)  # 2 docs per chunk\n",
        "medium_chunks = Chunk(data=docs, n=3)  # 3 docs per chunk  \n",
        "individual_chunks = Chunk(data=docs, n=1)  # 1 doc per chunk\n",
        "\n",
        "print(f\"Small chunks (n=2): {len(small_chunks)} chunks\")\n",
        "print(f\"Medium chunks (n=3): {len(medium_chunks)} chunks\")\n",
        "print(f\"Individual chunks (n=1): {len(individual_chunks)} chunks\")\n",
        "\n",
        "# Show what's in the chunks\n",
        "print(f\"\\nFirst small chunk: {len(small_chunks[0])} documents\")\n",
        "print(f\"First medium chunk: {len(medium_chunks[0])} documents\")\n",
        "\n",
        "# Use Recur to pass config to each chunk\n",
        "config = {\"include_topics\": True, \"verbose\": False}\n",
        "recurring_config = Recur(config, n=len(small_chunks))\n",
        "print(f\"\\nRecurring config will be passed to {len(recurring_config)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 — Parallel mapping with `async_process_map`\n",
        "\n",
        "**Goal:** Apply the worker to many chunks at once and measure the impact.\n",
        "\n",
        "* Run `async_process_map(insights, chunked_docs)` for two or three chunk sizes.\n",
        "* Print a short comparison: elapsed time and a count of processed documents.\n",
        "\n",
        "**What they learn:** `async_process_map` orchestrates parallel work, and chunk size matters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different chunk sizes with async_process_map\n",
        "async def test_chunk_performance():\n",
        "    # Reset counter\n",
        "    insights.total_processed = 0\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for name, chunks in [(\"small\", small_chunks), (\"medium\", medium_chunks), (\"individual\", individual_chunks)]:\n",
        "        start = time.time()\n",
        "        # Process all chunks in parallel\n",
        "        chunk_results = await async_process_map(insights, chunks)\n",
        "        elapsed = time.time() - start\n",
        "        \n",
        "        # Flatten results since we get a list of lists\n",
        "        all_results = []\n",
        "        for chunk_result in chunk_results:\n",
        "            if isinstance(chunk_result, list):\n",
        "                all_results.extend(chunk_result)\n",
        "            else:\n",
        "                all_results.append(chunk_result)\n",
        "        \n",
        "        results[name] = {\n",
        "            \"time\": elapsed,\n",
        "            \"chunks\": len(chunks),\n",
        "            \"docs\": len(all_results)\n",
        "        }\n",
        "    \n",
        "    return results\n",
        "\n",
        "perf_results = asyncio.run(test_chunk_performance())\n",
        "\n",
        "print(\"Chunk size comparison:\")\n",
        "for name, stats in perf_results.items():\n",
        "    print(f\"  {name}: {stats['chunks']} chunks, {stats['docs']} docs in {stats['time']:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 — Streaming results (`AsyncStreamProcess`)\n",
        "\n",
        "**Goal:** Show incremental output for long jobs.\n",
        "\n",
        "* Use `DocStream.astream(...)` on a longer text.\n",
        "* As results arrive, print a simple progress line (`chunk_id`, `%`, `running sentiment`) and then the final object.\n",
        "\n",
        "**What they learn:** Streaming delivers early feedback and fits long-running tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a longer document for streaming\n",
        "long_doc = \"\"\"\n",
        "Artificial intelligence represents one of the most transformative technologies of our time. \n",
        "The rapid advancement in machine learning has opened new possibilities. \n",
        "Data processing capabilities have grown exponentially in recent years. \n",
        "This technology brings both great opportunities and significant challenges. \n",
        "We must carefully consider the ethical implications of AI deployment. \n",
        "The benefits are clear in areas like healthcare and education. \n",
        "However, concerns about privacy and bias remain important topics. \n",
        "Overall, the future of AI looks promising with proper governance.\n",
        "\"\"\"\n",
        "\n",
        "# Stream the analysis\n",
        "stream_processor = DocStream()\n",
        "\n",
        "async def stream_analysis():\n",
        "    print(\"Streaming analysis...\")\n",
        "    \n",
        "    async for result in stream_processor.astream(long_doc):\n",
        "        if result[\"type\"] == \"chunk\":\n",
        "            print(f\"  Chunk {result['chunk_id']}: {result['progress']:.0f}% - Sentiment: {result['sentiment']}\")\n",
        "        else:  # final\n",
        "            print(f\"\\nFinal summary:\")\n",
        "            print(f\"  Dominant sentiment: {result['sentiment']}\")\n",
        "            print(f\"  All topics: {', '.join(result['topics'])}\")\n",
        "            print(f\"  Total words: {result['word_count']}\")\n",
        "            print(f\"  Chunks processed: {result['chunk_count']}\")\n",
        "\n",
        "asyncio.run(stream_analysis())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 — A small pipeline with `Sequential`\n",
        "\n",
        "**Goal:** Chain simple stages and keep data flowing.\n",
        "\n",
        "* Build `pipeline = Sequential([TextPrep(), DocInsights(), ResultShape()])`.\n",
        "* Send a few texts through the pipeline (sequentially first).\n",
        "* Reuse the **same** pipeline with `async_process_map` over `Chunk(data=docs, n=…)`.\n",
        "\n",
        "**What they learn:** Composition keeps pieces focused; the same stages work alone or in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a pipeline\n",
        "pipeline = Sequential([\n",
        "    TextPrep(),\n",
        "    DocInsights(),\n",
        "    ResultShape(format_type=\"summary\")\n",
        "])\n",
        "\n",
        "# Test on single documents\n",
        "print(\"Single document through pipeline:\")\n",
        "messy_doc = \"  This   is    a   great   example   with   extra   spaces.  \"\n",
        "result = asyncio.run(pipeline.aforward(messy_doc))\n",
        "print(f\"  Input: '{messy_doc}'\")\n",
        "print(f\"  Output: {result}\")\n",
        "\n",
        "# Now use the same pipeline with chunked data\n",
        "print(\"\\nChunked documents through pipeline:\")\n",
        "messy_docs = [\n",
        "    \"  AI  technology   is   amazing!  \",\n",
        "    \"  Data   science    provides   great   insights.  \",\n",
        "    \"  This  implementation   is   terrible.  \"\n",
        "]\n",
        "\n",
        "async def pipeline_chunks():\n",
        "    chunks = Chunk(data=messy_docs, n=2)\n",
        "    results = await async_process_map(pipeline, chunks)\n",
        "    return results\n",
        "\n",
        "chunk_results = asyncio.run(pipeline_chunks())\n",
        "print(f\"Processed {len(messy_docs)} documents in {len(chunk_results)} chunks\")\n",
        "for i, result in enumerate(chunk_results):\n",
        "    print(f\"  Chunk {i}: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7 — Aggregate results with `reduce` (or `async_reduce`)\n",
        "\n",
        "**Goal:** Summarize many batch results without loading everything into memory at once.\n",
        "\n",
        "* Create a very small aggregator (can be a one-purpose `Process` with `forward(acc, batch) -> acc`).\n",
        "* Use `reduce` or `async_reduce` to compute a simple summary (counts by sentiment, top topics).\n",
        "* Print the final summary.\n",
        "\n",
        "**What they learn:** Map → Reduce is the core pattern for large jobs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResultAggregator(Process):\n",
        "    \"\"\"Simple aggregator for document insights\"\"\"\n",
        "    \n",
        "    def forward(self, acc: Dict[str, Any], batch: Union[Dict, List]) -> Dict[str, Any]:\n",
        "        \"\"\"Aggregate batch results into accumulator\"\"\"\n",
        "        # Initialize accumulator if empty\n",
        "        if not acc:\n",
        "            acc = {\n",
        "                \"sentiment_counts\": {\"positive\": 0, \"negative\": 0, \"neutral\": 0},\n",
        "                \"all_topics\": set(),\n",
        "                \"total_words\": 0,\n",
        "                \"doc_count\": 0\n",
        "            }\n",
        "        \n",
        "        # Handle both single results and lists\n",
        "        results = [batch] if isinstance(batch, dict) else batch\n",
        "        \n",
        "        for result in results:\n",
        "            # Skip if it's a formatted string (from ResultShape)\n",
        "            if isinstance(result, str):\n",
        "                continue\n",
        "                \n",
        "            # Update counts\n",
        "            sentiment = result.get(\"sentiment\", \"neutral\")\n",
        "            acc[\"sentiment_counts\"][sentiment] += 1\n",
        "            acc[\"all_topics\"].update(result.get(\"topics\", []))\n",
        "            acc[\"total_words\"] += result.get(\"word_count\", 0)\n",
        "            acc[\"doc_count\"] += 1\n",
        "        \n",
        "        return acc\n",
        "\n",
        "# Analyze many documents\n",
        "many_docs = docs * 4  # 20 documents\n",
        "\n",
        "# Process in chunks and aggregate\n",
        "async def aggregate_analysis():\n",
        "    # Reset insights for fresh analysis\n",
        "    fresh_insights = DocInsights()\n",
        "    \n",
        "    # Process documents in chunks\n",
        "    chunks = Chunk(data=many_docs, n=5)\n",
        "    chunk_results = await async_process_map(fresh_insights, chunks)\n",
        "    \n",
        "    # Flatten results \n",
        "    all_results = []\n",
        "    for chunk_result in chunk_results:\n",
        "        if isinstance(chunk_result, list):\n",
        "            all_results.extend(chunk_result)\n",
        "        else:\n",
        "            all_results.append(chunk_result)\n",
        "    \n",
        "    # Aggregate using reduce\n",
        "    aggregator = ResultAggregator()\n",
        "    summary = reduce(aggregator, all_results, initial={})\n",
        "    \n",
        "    # Convert topics set to list for display\n",
        "    summary[\"all_topics\"] = list(summary[\"all_topics\"])\n",
        "    \n",
        "    return summary\n",
        "\n",
        "summary = asyncio.run(aggregate_analysis())\n",
        "\n",
        "print(\"Aggregate summary of 20 documents:\")\n",
        "print(f\"  Documents analyzed: {summary['doc_count']}\")\n",
        "print(f\"  Total words: {summary['total_words']}\")\n",
        "print(f\"  Average words/doc: {summary['total_words'] / summary['doc_count']:.1f}\")\n",
        "print(f\"  Sentiment distribution: {dict(summary['sentiment_counts'])}\")\n",
        "print(f\"  Topics found: {', '.join(summary['all_topics'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8 — Optional: plug in a real LLM (adapter only)\n",
        "\n",
        "**Goal:** Keep vendor isolation while showing where to hook it in.\n",
        "\n",
        "* Provide one small function `responses_call(prompt_or_batch) -> str|List[str]` following Tutorial 1's pattern.\n",
        "* Set `insights.caller = responses_call` and run **one** short example.\n",
        "* Keep the rest of the tutorial in offline mode.\n",
        "\n",
        "**What they learn:** Real APIs live behind one function. No API-chasing across the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of how to plug in a real LLM (commented out by default)\n",
        "\"\"\"\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "async def responses_call(text: Union[str, List[str]]) -> Union[str, List[str]]:\n",
        "    client = AsyncOpenAI()\n",
        "    \n",
        "    if isinstance(text, str):\n",
        "        response = await client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": f\"Analyze sentiment and topics: {text}\"}]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    else:\n",
        "        # Batch processing\n",
        "        tasks = [responses_call(t) for t in text]\n",
        "        return await asyncio.gather(*tasks)\n",
        "\n",
        "# To use it:\n",
        "# insights.caller = responses_call\n",
        "# result = await insights.aforward(\"Your text here\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"To use a real LLM, uncomment the code above and set:\")\n",
        "print(\"  insights.caller = responses_call\")\n",
        "print(\"\\nThe rest of the tutorial runs offline by default.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9 — Optional: mention serialization (no deep dive)\n",
        "\n",
        "**Goal:** Emphasize importance without turning this into a serialization lesson.\n",
        "\n",
        "* One sentence: \"Dachi's `spec()` and `state_dict()` let you save this pipeline's configuration and state.\"\n",
        "* Optionally show **one** quick `render(pipeline.spec())` snapshot (or skip entirely if we want the leanest flow).\n",
        "\n",
        "**What they learn:** The system can be saved and evolved later; details come in a dedicated tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dachi.core import render\n",
        "\n",
        "print(\"Dachi's `spec()` and `state_dict()` let you save this pipeline's configuration and state.\")\n",
        "print(\"\\nHere's a quick look at the pipeline structure:\")\n",
        "print(render(pipeline.spec())[:200] + \"...\")  # Just first 200 chars\n",
        "\n",
        "print(\"\\nSerialization enables saving/restoring your entire system. More details in a future tutorial!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrap-up: What you learned\n",
        "\n",
        "You've built a document insights pipeline that evolved from:\n",
        "1. **One async worker** → understanding `AsyncProcess`\n",
        "2. **Concurrent calls** → seeing async benefits for I/O\n",
        "3. **Chunking** → controlling batch sizes with `Chunk`\n",
        "4. **Parallel mapping** → scaling with `async_process_map`\n",
        "5. **Streaming** → getting incremental results with `AsyncStreamProcess`\n",
        "6. **Pipelines** → composing stages with `Sequential`\n",
        "7. **Aggregation** → summarizing with `reduce`\n",
        "\n",
        "Key takeaways:\n",
        "* **Async** for I/O-bound work (API calls, file operations)\n",
        "* **Chunking** to balance latency and throughput\n",
        "* **Streaming** for long-running tasks with progress\n",
        "* **Composition** to build complex systems from simple parts\n",
        "* **Vendor isolation** - swap LLMs by changing one function\n",
        "\n",
        "Next steps might include:\n",
        "* Error handling and retries\n",
        "* More complex aggregations\n",
        "* Custom streaming protocols\n",
        "* Production deployment patterns"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
